{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continent-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d023687",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "The data for this project comes from https://www.imdb.com/interfaces/ as extremely large .tsv (tab-seperated) files, the biggest being over 2GB.\n",
    "I'm handling this by loading each as a pandas dataframe, performing some simple data engineering in order to reduce the data so it doesn't crash my machine or take forever, then saving each as its own .csv file.\n",
    "These files are then read in and deeply wrangled before being merged and saved to a single .csv file.\n",
    "As of right now, there's a bit of SQL at the bottom of this file that I'm tinkering with.\n",
    "Also, the data dictionary on imdb.com is incorrect. I'll provide one once the data has been trimmed down and consolidated.\n",
    "\n",
    "### 1.1 Load & Inspect Each Table\n",
    "'usecols' is a useful parameter for speeding up the reading in of large files because I can specify just the columns I need pandas to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790a9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['tconst','nconst','category','ordering']\n",
    "principals = pd.read_csv('../Data/tsv/principals.tsv', sep='\\t',dtype='object', usecols=col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6212c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_list = ['tconst','titleType','primaryTitle','startYear','genres']\n",
    "basics = pd.read_table('../Data/tsv/basics.tsv', na_values=['\\\\N','nan'], dtype='object', usecols=col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e12b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_list = ['tconst', 'averageRating']\n",
    "ratings = pd.read_table('../Data/tsv/ratings.tsv', low_memory=False, na_values=['\\\\N','nan'], usecols=col_list)\n",
    "\n",
    "col_list = ['nconst', 'primaryName']\n",
    "name = pd.read_table('../Data/tsv/name.tsv', na_values=['\\\\N','nan'], usecols=col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e05bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_list = ['titleId','region']\n",
    "akas = pd.read_table('../Data/tsv/akas.tsv', na_values=['\\\\N','nan'], usecols=col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09965c69",
   "metadata": {},
   "source": [
    "#### 1.1.a - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fcf13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f734aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8827327 entries, 0 to 8827326\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   tconst        object\n",
      " 1   titleType     object\n",
      " 2   primaryTitle  object\n",
      " 3   startYear     object\n",
      " 4   genres        object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.7 GB\n",
      "startYear       1169788\n",
      "genres           403403\n",
      "primaryTitle          8\n",
      "titleType             0\n",
      "tconst                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "basics.info(memory_usage='deep')\n",
    "basics.head()\n",
    "print(basics.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b8706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606506\n",
      "275791\n",
      "262136\n",
      "211891\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 211891 entries, 61119 to 8827277\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   tconst        211891 non-null  object\n",
      " 1   primaryTitle  211891 non-null  object\n",
      " 2   startYear     211891 non-null  int64 \n",
      " 3   genres        211891 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 45.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>startYear</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61119</th>\n",
       "      <td>tt0062336</td>\n",
       "      <td>The Tango of the Widower and Its Distorting Mi...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62104</th>\n",
       "      <td>tt0063351</td>\n",
       "      <td>Summer in Narita</td>\n",
       "      <td>2012</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67672</th>\n",
       "      <td>tt0069049</td>\n",
       "      <td>The Other Side of the Wind</td>\n",
       "      <td>2018</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87119</th>\n",
       "      <td>tt0089067</td>\n",
       "      <td>El día de los albañiles 2</td>\n",
       "      <td>2001</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90923</th>\n",
       "      <td>tt0092960</td>\n",
       "      <td>En tres y dos</td>\n",
       "      <td>2004</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tconst                                       primaryTitle  \\\n",
       "61119  tt0062336  The Tango of the Widower and Its Distorting Mi...   \n",
       "62104  tt0063351                                   Summer in Narita   \n",
       "67672  tt0069049                         The Other Side of the Wind   \n",
       "87119  tt0089067                          El día de los albañiles 2   \n",
       "90923  tt0092960                                      En tres y dos   \n",
       "\n",
       "       startYear       genres  \n",
       "61119       2020        Drama  \n",
       "62104       2012  Documentary  \n",
       "67672       2018        Drama  \n",
       "87119       2001       Comedy  \n",
       "90923       2004        Drama  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trimming out tv shows and anything else that's not an actual movie.\n",
    "basics = basics[basics.titleType == 'movie']\n",
    "basics = basics.drop('titleType', axis=1)\n",
    "print(len(basics))\n",
    "\n",
    "# During an earlier view of the data I'd noticed that prior to 2000 there seem to be drastically fewer titles, drawing release year towards a left skew.\n",
    "basics = basics[basics.startYear.between('2000', '2022')]\n",
    "print(len(basics))\n",
    "\n",
    "#TODO just copy data without missing values here.\n",
    "basics.genres.replace('Nan',np.nan, inplace=True)\n",
    "basics.dropna(inplace=True)\n",
    "print(len(basics))\n",
    "\n",
    "# Many of the genre values are combinations of major genres, like drame, romance, and comedy. However, there's a ton of these, so I'll restrict the table to include only the 50 most frequently oberserved generes.\n",
    "genres = basics.genres.value_counts()[:-1]\n",
    "genres = genres[:50]\n",
    "top_genres = genres.index.to_list()\n",
    "basics = basics[basics['genres'].isin(top_genres)]\n",
    "print(len(basics))\n",
    "\n",
    "# Converting to numeric values for analysis.\n",
    "basics['startYear'] = pd.to_numeric(basics.startYear)\n",
    "\n",
    "####################### basics.to_csv('../Data/basics.csv', index=False)\n",
    "basics.info(memory_usage='deep')\n",
    "basics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3877a",
   "metadata": {},
   "source": [
    "#### 1.1.b - Principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353aa30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49755277 entries, 0 to 49755276\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   tconst    object\n",
      " 1   ordering  object\n",
      " 2   nconst    object\n",
      " 3   category  object\n",
      "dtypes: object(4)\n",
      "memory usage: 11.8 GB\n",
      "category    0\n",
      "nconst      0\n",
      "ordering    0\n",
      "tconst      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "principals.info(memory_usage='deep')\n",
    "principals.head()\n",
    "print(principals.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1856faca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actor                  11060267\n",
       "self                    8679029\n",
       "actress                 8497760\n",
       "writer                  6846908\n",
       "director                5727872\n",
       "producer                3200745\n",
       "cinematographer         1719987\n",
       "composer                1715085\n",
       "editor                  1646282\n",
       "production_designer      340973\n",
       "archive_footage          317221\n",
       "archive_sound              3148\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principals.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fca07a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Limiting this table to a set of the most frequent roles.\n",
    "principal_roles = ['actor','actress','director','writer','producer','composer']\n",
    "principals['ordering'] = pd.to_numeric(principals.ordering)\n",
    "principals = principals[(principals.ordering == 1) & (principals.category.isin(principal_roles))]\n",
    "print(len(principals))\n",
    "principals.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############principals.to_csv('../Data/principals.csv', index=False)\n",
    "principals.info(memory_usage='deep')\n",
    "principals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fad43",
   "metadata": {},
   "source": [
    "#### 1.1.c - Ratings\n",
    "I only need to scale this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ea8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings.isna().sum().sort_values(ascending=False))\n",
    "############ratings.to_csv('../Data/ratings.csv', index=False)\n",
    "ratings.info(memory_usage='deep')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1aedcd",
   "metadata": {},
   "source": [
    "#### 1.1.d - Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name.isna().sum().sort_values(ascending=False))\n",
    "name.info(memory_usage='deep')\n",
    "name.head()\n",
    "\n",
    "##################### name.to_csv('../Data/name.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a00156",
   "metadata": {},
   "source": [
    "#### 1.1.e - akas\n",
    "I just need the region for each individual film so I can reduce the impact of running ohe on either/both movie tiles and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e72fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(akas.isna().sum().sort_values(ascending=False))\n",
    "akas.rename({'titleId': 'tconst'}, axis=1, inplace=True)\n",
    "len(akas)\n",
    "\n",
    "# I just want U.S. films\n",
    "akas = akas[akas.region == 'US']\n",
    "\n",
    "# memory usage has been drastically reduced for this table, hopefully it reflects when I use the set of tconst values to filter the basics tables prior to the merge.\n",
    "akas.info(memory_usage='deep')\n",
    "akas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898a21",
   "metadata": {},
   "source": [
    "### 1.2 Merging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling casing now that each table is ready for the merger.\n",
    "ratings.columns = map(str.lower, ratings.columns)\n",
    "name.columns = map(str.lower, name.columns)\n",
    "principals.columns = map(str.lower, principals.columns)\n",
    "basics.columns = map(str.lower, basics.columns)\n",
    "\n",
    "# filtering basics down to only movies from the 'US' region. This'll greatly reduced everything being add from the other tables.\n",
    "tconst = list(set(akas.tconst.values))\n",
    "# basics2 = basics[basics['tconst'].isin(tconst)]\n",
    "basics = basics.merge(akas, how='left',on='tconst')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21923ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# simple merge\n",
    "data = basics.merge(principals, how='left',on='tconst')\n",
    "data = data.merge(ratings, how='left',on='tconst')\n",
    "data = data.merge(name, how='left',on='nconst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2149c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.averagerating.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80341561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['tconst','nconst'],axis=1,inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.to_csv('../Data/data.csv', index=False)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5e96d",
   "metadata": {},
   "source": [
    "### 1.3 Table Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbee8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/data.csv')\n",
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data dictionary\n",
    "tconst  =   title id of the movie\n",
    "primarytitle    =   primary title the movie goes by\n",
    "startyear   =   year realease\n",
    "runtimeminutes  =   film duration\n",
    "genres  =   list of each genre the film represents\n",
    "ordering    =   order of precedence if co-directors/writers/producers\n",
    "nconst  =   name id or director, writer\n",
    "category    =   job category7\n",
    "primaryname =   director/writer name gone by\n",
    "primaryprofession   =   primary postion of principal\n",
    "knownfortitles  =   previous works by principle\n",
    "averagerating   =   films average rating\n",
    "numvotes    =   number of votes film has received\n",
    "directors   =   list of directors\n",
    "writers =   list of writers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplcates.\n",
    "data[data.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just doublechecking.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258390a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and Modify columns\n",
    "data.columns\n",
    "# The columns are already formatted to lowercase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea7f31",
   "metadata": {},
   "source": [
    "## 2. Initial EDA: Feature Selection\n",
    "### 2.1 Data Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8939df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')\n",
    "data.describe(include='all')\n",
    "\n",
    "print(f'Number of dupes: {sum(data.duplicated())}')\n",
    "data[data.isnull().any(axis=1)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ad815",
   "metadata": {},
   "source": [
    "### 2.2 Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814da602",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d221c",
   "metadata": {},
   "source": [
    "#### 2.2.a - primarytitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.primarytitle.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9e55d",
   "metadata": {},
   "source": [
    "#### 2.2.b - genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609941a5",
   "metadata": {},
   "source": [
    "#### 2.2.c - category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439ee41",
   "metadata": {},
   "source": [
    "#### 2.2.d - primaryname\n",
    "This is a huge source of dimensionality. For now I'm simply going to drop anyone who appears only once. This is hand during the .tsv file conversion process further up but I may fine tune here in the the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f978713",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.primaryname.duplicated(keep=False)]\n",
    "print(data.primaryname.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73158427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "for f in data[['genres','category']]:\n",
    "    sns.countplot(x = f, data = data[data.ordering == 1], palette = 'Set3') # hue = '')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73d51a",
   "metadata": {},
   "source": [
    "### 2.3 Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf405bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('number').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3c6a8",
   "metadata": {},
   "source": [
    "### 2.3.a - startyear\n",
    "- The average start year for the films in this selection is 2009.\n",
    "- This distribution should be plotted with lines indicating centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15081921",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data.startyear)\n",
    "plt.show()\n",
    "sns.kdeplot(data.startyear, shade=True, label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df307",
   "metadata": {},
   "source": [
    "### 2.3.b - averagerating\n",
    "- This is likely to be some sort of target in the future, linear regression would be great to take this project a step further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data.averagerating)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further exploring the outlier impact.\n",
    "sns.kdeplot(data.averagerating, shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy I'll first calculate the IQR, then use it to identify and remove outliers found in the averagerating feature.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#TODO come back and trim this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming off everything above and below the threshold.\n",
    "# Intuition on this dictates that we want data that extreme outliers can lead to groupings - ansd their centroids, being dragged out due to these skewed data.\n",
    "data = data[data.averagerating >= iqr_lower]\n",
    "data = data[data.averagerating <= iqr_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "# we've lost only a small number of rows.\n",
    "#TODO get the original number and show difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out the new distribution using the previous distributions IQR method ranges.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0866173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checking out the new distribution using the new IQR.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.averagerating > iqr_lower]\n",
    "data = data[data.averagerating < iqr_upper]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data.averagerating, shade=True, label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out these new summary stats\n",
    "# the max is a more realistic two hours or so while the mean remains about the same. The standard deviation has also been halved.\n",
    "data.averagerating.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-sunrise",
   "metadata": {},
   "source": [
    "### 2.4 Feature Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=data,x=data.category,y=data.averagerating)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=data,x=data.genres,y=data.averagerating)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.show()\n",
    "#TODO sort this and amke wider for x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09857ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'averagerating'\n",
    "def find_associations(data):\n",
    "    associated = []\n",
    "    for i in data.select_dtypes(np.number).columns:\n",
    "        print(i)\n",
    "        if i == target:\n",
    "            continue\n",
    "        pearson_cor, pval = pearsonr(data[i],data[target])\n",
    "\n",
    "        if pearson_cor > .3:\n",
    "            associated.append([i,pearson_cor])\n",
    "    return associated\n",
    "\n",
    "# To do, there is no target, should I drop this? Or, could it be useful in evaluation..?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-links",
   "metadata": {},
   "source": [
    "## 3. Feature Selection & Hyperparameter Tuning\n",
    "After checking a range of cluster quantities I'm going to use principal component analysis from Sklearn to to reduce the dimensionality of the data. In fact, one hot encoding is used in the next cell \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ed8e9",
   "metadata": {},
   "source": [
    "#### 3.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = ['genres','category','primaryname']\n",
    "scal_cols = ['startyear','averagerating']\n",
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21ab7d",
   "metadata": {},
   "source": [
    "#### 3.2 Feature Encoding\n",
    "I'm using a column transformer to encode the data, x_train, that I can use for both finding the optimal k and also conducting PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #('le', OrdinalEncoder(), le),\n",
    "        ('ohe', OneHotEncoder(handle_unknown ='ignore'), ohe),\n",
    "        ('scaler', StandardScaler(), scal_cols)\n",
    "        ],remainder='drop')\n",
    "\n",
    "\n",
    "x_train = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7ffa8",
   "metadata": {},
   "source": [
    "#### 3.3 Optimal K: Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "c_dict = {}\n",
    "n_clusters = [range(1, 11)]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "    kmeans.fit(x_train)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    if k not in c_dict.keys():\n",
    "        c_dict[k] = kmeans.inertia_\n",
    "\n",
    "    print(\"The innertia for :\", k, \"Clusters is:\", kmeans.inertia_)\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distance (Inertia)')\n",
    "plt.title(\"Inertia Plot for k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96408d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 20, n_init = 8, random_state = 42)\n",
    "kmeans.fit(x_train)\n",
    "labels = set(kmeans.labels_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(x_train)\n",
    "kmeans.inertia_\n",
    "old_inertia = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just looking to see what the training data looks like. So ohe columns..\n",
    "pd.DataFrame(x_train.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886ea0b",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [f'Cluster {x}' for x in labels]\n",
    "target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train.toarray()\n",
    "y_pred = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d98415",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 4\n",
    "pca = PCA(n_components=n_components, random_state = 42)\n",
    "X_r = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a65f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['navy', 'turquoise', 'darkorange', 'red', 'black']\n",
    "for color, i, target_name in zip(colors[:len(target_labels)], list(range(len(target_labels))), target_labels):\n",
    "    plt.scatter(X_r[y_pred == i, 0], X_r[y_pred == i, 1], color=color, alpha=.8, lw=2,label=target_name)\n",
    "    \n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.6)   \n",
    "plt.title(f'PCA of {n_components} Items')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d335465",
   "metadata": {},
   "source": [
    "Determining Optimal Number of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = x_train.shape[1]//2\n",
    "pca = PCA(n_components=n_components, random_state = 42)\n",
    "X_r = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_variance = sum(pca.explained_variance_)\n",
    "print('Total Variance in our dataset is: ', total_variance)\n",
    "var_95 = total_variance * 0.95\n",
    "print('The 95% variance we want to have is: ', var_95)\n",
    "print('')\n",
    "# Creating a df with the components and explained variance\n",
    "a = zip(range(0,n_components), pca.explained_variance_)\n",
    "a = pd.DataFrame(a, columns=['PCA Comp', 'Explained Variance'])\n",
    "\n",
    "# Trying to hit 95%\n",
    "\n",
    "d = 1\n",
    "v = []\n",
    "best = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    if len(v) > len(a)*.9:\n",
    "        if sum(v[-5:])/5 == v[:-1]:\n",
    "            break\n",
    "    else:\n",
    "        v.append(sum(a['Explained Variance'][0:d]))\n",
    "        if d%5 == 0:\n",
    "            print(f'Variance explained with {d} compononets: ', sum(a['Explained Variance'][0:d]))\n",
    "            if sum(a['Explained Variance'][0:d]) >= var_95:\n",
    "                best.append((d,sum(a['Explained Variance'][0:d])))\n",
    "        d += 1\n",
    "\n",
    "\n",
    "best_c = best[0][0]\n",
    "best_v = best[0][1]\n",
    "\n",
    "\n",
    "# Plotting the Data\n",
    "plt.figure(1, figsize=(14, 8))\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2, c='r')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_ratio_')\n",
    "\n",
    "# Plotting line with 95% e.v.\n",
    "plt.axvline(best_c,linestyle=':', label='n_components - 95% explained', c ='blue')\n",
    "plt.legend(prop=dict(size=12))\n",
    "\n",
    "# adding arrow\n",
    "plt.annotate(f'{best_c} eigenvectors used to explain 95% variance', xy=(best_c, pca.explained_variance_ratio_[best_c]), \n",
    "             xytext=(best_c+10, pca.explained_variance_ratio_[5]),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'The best is {best_c} components which yeilds {best_v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b464e6",
   "metadata": {},
   "source": [
    "Using PCA with this optimal number of components to add a preprocessing layer to the data before applying KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=best_c)\n",
    "X_r = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "cs = []\n",
    "n_clusters = [range(1, 11)]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "    kmeans.fit(X_r)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    if k not in c_dict.keys():\n",
    "        c_dict[k] = kmeans.inertia_\n",
    "\n",
    "    print(\"The innertia for :\", k, \"Clusters is:\", kmeans.inertia_)\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distance (Inertia)')\n",
    "plt.title(\"Inertia Plot for k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "kmeans.fit(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(X_r)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_pred'] = y_pred\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.inertia_)\n",
    "print(old_inertia)\n",
    "print(old_inertia-kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sqlalchemy import create_engine\n",
    "engine = create_engine(\"mysql://user:pwd@localhost/kmeans\",echo = True)\n",
    "data.to_sql('kmeans', schema='dbo', con = engine, if_exists = 'replace')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb310d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.clustermap(data=data[['startyear','averagerating']])\n",
    "# plt.show()\n",
    "#TODO check out this 'fastcluster' thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../Data/s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d22ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2f5eeafb4738538680cc9389be83c7f54c751ee609fa97da0b7e381233777c0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
