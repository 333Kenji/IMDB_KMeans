{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continent-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from to_img import to_img\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "colors = ['navy', 'turquoise', 'brown', 'red', 'black','green', 'orange', 'pink']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d023687",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "The data for this project comes from https://www.imdb.com/interfaces/ as extremely large .tsv (tab-seperated) files, the biggest being over 2GB. In order to conduct train the KMeans++ model I will need to combine all of this data. But first, in order to load it without overloading my system I need to apply some data wranging and engineering. The need for each of these files is briefly explained below.\n",
    "\n",
    "- akas: I need region data so I can narrow the data to 'US' based films.\n",
    "- basics: contains movie specific info\n",
    "- name: contains names of cast and crew along with their reference code (nconst)\n",
    "- principals: contains order of precedence if more than one nconst (person) is referenced to the same role (i.e. lead vs co-directors)\n",
    "- ratings: holds IMDB user rating and vote counts\n",
    "\n",
    "I'm handling this huge memory load by using pandas to read load the data in chunk, and filtering out specific columns and values that are irrelevant to the project. The data is then saved to a .csv file to ensure stability (of my machine) that I load back in.\n",
    "\n",
    "As of right now, there's a bit of SQL at the bottom of this file that I'm tinkering with.\n",
    "Also, the data dictionary on imdb.com is incorrect. I'll provide one once the data has been trimmed down and consolidated.\n",
    "\n",
    "### 1.1 Load and Merge Tables\n",
    "'usecols' is a useful parameter for speeding up the reading in of large files because I can specify just the columns I need pandas to parse.\n",
    "\n",
    "In .read_csv iterator and chunksize let me specify how much data from each .tsv file will be preprocessed before being recomiled into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f027a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(path,cols):\n",
    "    \"\"\"\n",
    "    Summary: loads .tsv files in chunks, selecting specific features and filtering unnecessary values.\n",
    "        _\n",
    "    Args:\n",
    "        path (string): local address\n",
    "        cols (list): features to keep\n",
    "    Returns:\n",
    "       df_result (pd.DataFrame): preprocessed dataframe\n",
    "    \"\"\"\n",
    "    PATH = path\n",
    "    my_chunk = 100000\n",
    "    # sets up the size and parameters of the file reader\n",
    "    iter_csv = pd.read_csv(\n",
    "        PATH,\n",
    "        na_values=['\\\\N','nan'],\n",
    "        delimiter='\\t',\n",
    "        iterator=True,\n",
    "        dtype='object',\n",
    "        chunksize=my_chunk,\n",
    "        usecols=cols)\n",
    "    \n",
    "    # the following logic follows that when the function is called, if the string arguement passed through as the path parameter matches the case string, a speciic data engineering process is executed for that specific data.\n",
    "    if 'rating' in PATH:\n",
    "        df_result = pd.concat([chunk for chunk in iter_csv])\n",
    "        #df_result = pd.concat([chunk[chunk.averageRating.astype('float') > 5] for chunk in iter_csv])\n",
    "    elif 'name' in PATH:\n",
    "        df_result = pd.concat([chunk for chunk in iter_csv])\n",
    "    elif 'principals' in PATH:\n",
    "        # there's a number of roles that barely appear in the data.\n",
    "        #TODO roll these into an 'other'\n",
    "        principal_roles = ['actor', 'actress','director','writer','producer','composer']\n",
    "        df_result = pd.concat(\n",
    "            [chunk[(chunk['ordering'].astype('int') <= 3) & (chunk['category'].isin(principal_roles))]\n",
    "            for chunk in iter_csv])\n",
    "        \n",
    "    elif 'akas' in PATH:\n",
    "        df_result = pd.concat([chunk[chunk.region == 'US'] for chunk in iter_csv])\n",
    "        df_result.rename({'titleId': 'tconst'}, axis=1, inplace=True)\n",
    "    elif 'basics' in PATH:\n",
    "        # Many films have a list of genres which explodes in dimensionality when one hot encoding. So instead, I'm sticking with the films that're 'quisessentially' a specific genre.\n",
    "        principle_genres = ['Drama','Comedy','Adventure','Action', 'Horror', 'Thriller', 'Fantasy', 'Crime', 'Family', 'Sci-Fi', 'Mystery',\n",
    "            'Romance']\n",
    "        # I also observed a high skew to startYear values so I'm also limiting the scope of the data to just the past 2+ decades.\n",
    "        df_result = pd.concat([chunk[\n",
    "            (chunk.titleType == 'movie')\n",
    "            & (chunk.startYear.between('2000', '2022'))\n",
    "            & (chunk['genres'].isin(principle_genres))] for chunk in iter_csv])\n",
    "        \n",
    "        df_result.startYear = pd.to_numeric(df_result.startYear)\n",
    "        df_result.runtimeMinutes = pd.to_numeric(df_result.runtimeMinutes)\n",
    "        df_result = df_result[df_result.runtimeMinutes >= 70]\n",
    "        df_result.runtimeMinutes = pd.to_numeric(df_result.runtimeMinutes)\n",
    "        df_result = df_result.drop('titleType', axis=1)\n",
    "        \n",
    "    df_result.columns = map(str.lower, df_result.columns)\n",
    "\n",
    "    return pd.DataFrame(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ddc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Each .tsv file is loaded by calling the chunks function using a list of its columns and the local address\n",
    "\n",
    "col_list = ['tconst','titleType','primaryTitle','startYear','genres','runtimeMinutes']\n",
    "basics_tsv = '../Data/tsv/basics.tsv'\n",
    "basics = pd.DataFrame(chunks(basics_tsv,col_list))\n",
    "\n",
    "\n",
    "col_list = ['titleId','region']\n",
    "akas_tsv = '../Data/tsv/akas.tsv'\n",
    "akas = pd.DataFrame(chunks(akas_tsv,col_list))\n",
    "\n",
    "\n",
    "col_list = ['tconst','ordering','nconst','category'\t]\n",
    "principals_tsv = '../Data/tsv/principals.tsv'\n",
    "principals = pd.DataFrame(chunks(principals_tsv,col_list))\n",
    "\n",
    "\n",
    "col_list = ['nconst', 'primaryName']\n",
    "names_tsv = '../Data/tsv/name.tsv'\n",
    "names = pd.DataFrame(chunks(names_tsv,col_list))\n",
    "\n",
    "\n",
    "col_list = ['tconst','averageRating','numVotes']\n",
    "ratings_tsv = '../Data/tsv/ratings.tsv'\n",
    "ratings = pd.DataFrame(chunks(ratings_tsv,col_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883adfe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-94f99aef3d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbasics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0makas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprincipals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../Images/01_data/head_{head}.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/BasicProjects/KMeans_Numbers/Notebooks/to_img.py\u001b[0m in \u001b[0;36mto_img\u001b[0;34m(data, col_width, row_height, font_size, header_color, row_colors, edge_color, bbox, header_columns, ax, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m        \u001b[0mdf_result\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "heads = [basics.head(), akas.head(), principals.head(), names.head(), ratings.head()]\n",
    "for head in heads:\n",
    "    fig = to_img(head, header_columns=0, col_width=2.5)\n",
    "    fig.savefig(f'../Images/01_data/head_{head}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898a21",
   "metadata": {},
   "source": [
    "### 1.2 Merging Tables\n",
    "First, the basics table is filtered by a list of tconst (film specific) values for films that were identified in the akas table as being from the 'US'.\n",
    "Then the tconst and nconst (person specific) keys are used to merge each table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtering basics down to only movies from the 'US' region. This'll greatly reduced everything being add from the other tables.\n",
    "tconst = list(set(akas.tconst.values))\n",
    "basics = basics[basics['tconst'].isin(tconst)]\n",
    "\n",
    "data = basics.merge(principals, how='left',on='tconst')\n",
    "data = data.merge(ratings, how='left',on='tconst')\n",
    "data = data.merge(names, how='left',on='nconst')\n",
    "\n",
    "data.drop(['nconst'],axis=1,inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.to_csv('../Data/data.csv', index=False)\n",
    "print(len(data))\n",
    "print(data.info(memory_usage='deep'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5e96d",
   "metadata": {},
   "source": [
    "### Reload and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbee8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the change in memory usage.\n",
    "data = pd.read_csv('../Data/data.csv')\n",
    "print(len(data))\n",
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data dictionary\n",
    "tconst  =   title id of the movie\n",
    "primarytitle    =   primary title the movie goes by\n",
    "startyear   =   year realease\n",
    "runtimeminutes  =   film duration\n",
    "genres  =   list of each genre the film represents\n",
    "ordering    =   order of precedence if co-directors/writers/producers\n",
    "nconst  =   name id or director, writer\n",
    "category    =   job category7\n",
    "primaryname =   director/writer name gone by\n",
    "primaryprofession   =   primary postion of principal\n",
    "knownfortitles  =   previous works by principle\n",
    "averagerating   =   films average rating\n",
    "numvotes    =   number of votes film has received\n",
    "directors   =   list of directors\n",
    "writers =   list of writers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplcates.\n",
    "print(f'Number of dupes: {sum(data.duplicated())}')\n",
    "data[data.duplicated() == True]\n",
    "\n",
    "#data.drop_duplicates(inplace=True)\n",
    "#data[data.duplicated() == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just doublechecking.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea7f31",
   "metadata": {},
   "source": [
    "## 2. Initial EDA: Features\n",
    "### 2.1 Data Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8939df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ad815",
   "metadata": {},
   "source": [
    "### 2.2 Categorical Features\n",
    "I have to pay special attention here. Note that 'primarytitle' and 'primaryname' would dramatically insrease the dimensionality of the data. Something that I noticed in an earlier iteration is that 'primary' titles are sometimes duplicated, which makes sense because some movies share their title with another, such as in the case of remakes. Therefore, I don't think primary title is a feature I should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814da602",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83efac8",
   "metadata": {},
   "source": [
    "##### 2.2.a - tsconst\n",
    "This feature plays a huge factor in my analysis because it's the marker for distinct movies. Sometimes a 'tconst' value will appear multiple times, this happens because some movies have mutiple people associated with it; so a film with a director, actor, and actress will have three seperate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13070b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tconst.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d221c",
   "metadata": {},
   "source": [
    "##### 2.2.b - primarytitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a136208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.primarytitle.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9e55d",
   "metadata": {},
   "source": [
    "##### 2.2.c - genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.genres.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609941a5",
   "metadata": {},
   "source": [
    "##### 2.2.d - category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f279d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further reducing the number of possible categorical values.\n",
    "keeps = ['actor', 'actress', 'director']\n",
    "data = data[data.category.isin(keeps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439ee41",
   "metadata": {},
   "source": [
    "##### 2.2.e - primaryname\n",
    "This is a huge source of dimensionality. For now I'm simply going to drop anyone who appears only once. This is hand during the .tsv file conversion process further up but I may fine tune here in the the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f978713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[data.groupby(\"primaryname\")['primaryname'].transform('size') > 5]\n",
    "print(data.primaryname.value_counts())\n",
    "data = data[data.groupby(\"primaryname\")['primaryname'].transform('size') > 1]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a60aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.primaryname.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73158427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so much drama...\n",
    "for f in data[['genres','category']]:\n",
    "    sns.countplot(x = f, data = data, palette = 'Set3') # hue = '')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.savefig(f'../Images/cat_countplot_{f}.jpg')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73d51a",
   "metadata": {},
   "source": [
    "### 2.3 Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = data.describe()\n",
    "data.select_dtypes('number').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3c6a8",
   "metadata": {},
   "source": [
    "##### 2.3.a - startyear\n",
    "- The average start year for the films in this selection is 2009.\n",
    "- This distribution should be plotted with lines indicating centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15081921",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data.startyear)\n",
    "plt.savefig(f'../Images/num_histplot_startyear.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.kdeplot(data.startyear, shade=True, label='data')\n",
    "plt.savefig(f'../Images/num_kdeplot_startyear.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df307",
   "metadata": {},
   "source": [
    "##### 2.3.b - averagerating\n",
    "- This is likely to be some sort of target in the future, linear regression would be great to take this project a step further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data.averagerating)\n",
    "plt.savefig(f'../Images/num_histplot_averagerating.jpg')\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(data.averagerating, shade=True, label='data')\n",
    "plt.savefig(f'../Images/num_kde_averagerating.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy I'll first calculate the IQR, then use it to identify and remove outliers found in the averagerating feature.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "plt.legend()\n",
    "plt.savefig(f'../Images/outliers_check_dis_averagerating.jpg')\n",
    "plt.show()\n",
    "#TODO come back and trim this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming off everything above and below the threshold.\n",
    "# Intuition on this dictates that we want data that extreme outliers can lead to groupings - ansd their centroids, being dragged out due to these skewed data.\n",
    "data = data[data.averagerating >= iqr_lower]\n",
    "data = data[data.averagerating <= iqr_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "# we've lost only a small number of rows.\n",
    "#TODO get the original number and show difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0866173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checking out the new distribution using the new IQR.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(f'../Images/outliers_recheck_dis_averagerating.jpg')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.averagerating > iqr_lower]\n",
    "data = data[data.averagerating < iqr_upper]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data.averagerating, shade=True, label='data')\n",
    "plt.savefig(f'../Images/num_kde_cleaned_averagerating.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out these new summary stats\n",
    "# the max is a more realistic two hours or so while the mean remains about the same. The standard deviation has also been halved.\n",
    "data.averagerating.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-sunrise",
   "metadata": {},
   "source": [
    "### 2.4 Feature Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=data,x=data.category,y=data.averagerating)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=data,x=data.genres,y=data.averagerating)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.show()\n",
    "#TODO sort this and amke wider for x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488cfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.primarytitle.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-links",
   "metadata": {},
   "source": [
    "## 3. Feature Selection & Hyperparameter Tuning\n",
    "After checking a range of cluster quantities I'm going to use principal component analysis from Sklearn to to reduce the dimensionality of the data. To deal with the categorical variables I'm going to encode them ordinally so can feed the information to the KNN++ model without having to draatically increase me feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ed8e9",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Feature Selection\n",
    "\n",
    "A key concept with PCA is that the first features considered matter the most - I'd originally struggled to get good metrics because of how I was feeding data in via the pipeline.\n",
    "primaryname explodes when one hot encoded, primarytitle follows. Since I reduced genres down to a list of quintessential genres (i.e. not a list of genres. \"Drama\" vs \"['Drama', 'Horror']\").\n",
    "In the cell below I'm setting up to run summary analysis on numeric features. Noting the difference in quantity between the appearance of unique values, I'm arranging the pipeline of their unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ccbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = ['category','genres','tconst','primaryname']\n",
    "for i in ohe:\n",
    "    print(f'{i} {data[i].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserving data so I can add clusters identified by KMeans++ as a new column later. \n",
    "X = data\n",
    "ordi = OrdinalEncoder()\n",
    "X['name'] = ordi.fit_transform(pd.DataFrame(X.primaryname))\n",
    "X['const'] = ordi.fit_transform(pd.DataFrame(X.tconst))\n",
    "\n",
    "# feature encoding\n",
    "ohe = ['category','genres']\n",
    "scal_cols = ['startyear','averagerating','numvotes','runtimeminutes','name','const']\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21ab7d",
   "metadata": {},
   "source": [
    "### 3.2 Feature Encoding\n",
    "I'm using a column transformer to encode the X, which is the same as data but with a selection of categorical features ordinally encoded. It will return x_train which I'll then use for finding the optimal number of centroids (k) and components (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #('ordi', OrdinalEncoder(), ordi),\n",
    "        ('scaler', StandardScaler(), scal_cols),\n",
    "        ('ohe', OneHotEncoder(handle_unknown ='ignore'), ohe)\n",
    "        \n",
    "        ],remainder='drop')\n",
    "\n",
    "\n",
    "x_train = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1806c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just looking to see what the training data looks like.\n",
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7ffa8",
   "metadata": {},
   "source": [
    "### 3.3 Optimal K: Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "c_dict = {}\n",
    "n_clusters = [range(1, 15)]\n",
    "for k in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 30, n_init = 4, random_state = 42)\n",
    "    kmeans.fit(x_train)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    if k not in c_dict.keys():\n",
    "        c_dict[k] = kmeans.inertia_\n",
    "\n",
    "    print(\"The inertia for :\", k, \"Clusters is:\", kmeans.inertia_)\n",
    "plt.plot(range(1, 15), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distance (Inertia)')\n",
    "plt.title(\"Inertia Plot for k\")\n",
    "plt.savefig('../Images/hypertuning_kmeans_elbow1.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96408d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best k shown by the elbow method to train a new model.\n",
    "kmeans = KMeans(n_clusters = 6, init = 'k-means++', max_iter = 20, n_init = 8, random_state = 42)\n",
    "kmeans.fit(x_train)\n",
    "\n",
    "# saving for PCA\n",
    "labels = set(kmeans.labels_)\n",
    "# saving for PCA\n",
    "y_pred = kmeans.predict(x_train)\n",
    "# saving this to compare with final model\n",
    "old_inertia = kmeans.inertia_\n",
    "# labels for plot legend\n",
    "target_labels = [f'Cluster {1+x}' for x in labels]\n",
    "print(f'Model Inertia: {kmeans.inertia_}')\n",
    "print(f'Model Label: {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe4bce",
   "metadata": {},
   "source": [
    "### 3.4 Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d98415",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pca = PCA(n_components=n_components, random_state = 42)\n",
    "X_r = pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14, 8))\n",
    "for color, i, target_name in zip(colors[:len(target_labels)], list(range(len(target_labels))), target_labels):\n",
    "    plt.scatter(X_r[y_pred == i, 0], X_r[y_pred == i, 1], color=color, alpha=.8, lw=2,label=target_name)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.6)   \n",
    "plt.title(f'PCA of {n_components} Items')\n",
    "plt.savefig('../Images/hypertuning_PCA_first_2.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d335465",
   "metadata": {},
   "source": [
    "#### 3.4.1 Finding Optimal N-Components Using Optimal K\n",
    "In order to find the optimal number of components to abstract the feature set into I need to analyze the entire table to find both the total variance and its 95% percentile. The number of components that can reduce variance by 5% is what we'll go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = x_train.shape[1]\n",
    "pca = PCA(n_components=n_components, random_state = 42)\n",
    "X_r = pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_variance = sum(pca.explained_variance_)\n",
    "print('Total Variance in our dataset is: ', total_variance)\n",
    "var_95 = total_variance * 0.95\n",
    "print('The 95% variance we want to have is: ', var_95)\n",
    "print('')\n",
    "# Creating a df with the components and explained variance\n",
    "a = zip(range(0,n_components), pca.explained_variance_)\n",
    "a = pd.DataFrame(a, columns=['PCA Comp', 'Explained Variance'])\n",
    "\n",
    "# Trying to hit 95%\n",
    "\n",
    "d = 1\n",
    "v = []\n",
    "best = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    if len(v) > a.shape[0]*.9:\n",
    "        if sum(v[-5:])/5 == v[:-1]:\n",
    "            break\n",
    "    else:\n",
    "        v.append(sum(a['Explained Variance'][0:d]))\n",
    "        if d%5 == 0:\n",
    "            print(f'Variance explained with {d} compononets: ', sum(a['Explained Variance'][0:d]))\n",
    "            if sum(a['Explained Variance'][0:d]) >= var_95:\n",
    "                best.append((d,sum(a['Explained Variance'][0:d])))\n",
    "        d += 1\n",
    "\n",
    "\n",
    "best_c = best[0][0]\n",
    "best_v = best[0][1]\n",
    "\n",
    "\n",
    "# Plotting the Data\n",
    "plt.figure(1, figsize=(14, 8))\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2, c='r')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_ratio_')\n",
    "\n",
    "# Plotting line with 95% e.v.\n",
    "plt.axvline(best_c,linestyle=':', label='n_components - 95% explained', c ='blue')\n",
    "plt.legend(prop=dict(size=12))\n",
    "\n",
    "# adding arrow\n",
    "plt.annotate(f'{best_c} eigenvectors used to explain 95% variance', xy=(best_c, pca.explained_variance_ratio_[best_c]), \n",
    "             xytext=(best_c+10, pca.explained_variance_ratio_[5]),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "plt.savefig('../Images/hypertuning_PCA_explanatory_components.jpg')\n",
    "plt.show()\n",
    "\n",
    "print(f'The best is {best_c} components which yields {best_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b464e6",
   "metadata": {},
   "source": [
    "#### 3.4.2 Rechecking For Optimal K-Means Using Optimal N-Components\n",
    "Using PCA with this optimal number of components to add a preprocessing layer to the data before applying KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=best_c)\n",
    "X_r = pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "cs = []\n",
    "n_clusters = [range(1, 11)]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "    kmeans.fit(X_r)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    if k not in c_dict.keys():\n",
    "        c_dict[k] = kmeans.inertia_\n",
    "\n",
    "    print(\"The inertia for :\", k, \"Clusters is:\", kmeans.inertia_)\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distance (Inertia)')\n",
    "plt.title(\"Inertia Plot for k\")\n",
    "plt.savefig('../Images/hypertuning_kmeans_elbow2.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009d70a",
   "metadata": {},
   "source": [
    "### 4. Training KMeans ++ with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 6, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "kmeans.fit(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(X_r)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cluster'] = y_pred\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd227e",
   "metadata": {},
   "source": [
    "### 5. KMeans++ Model and PCA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'old inertia: {old_inertia}')\n",
    "print(f'new inertia: {kmeans.inertia_}')\n",
    "print(f'change:      {old_inertia-kmeans.inertia_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(kmeans.labels_)\n",
    "target_labels = [f'Cluster {x}' for x in labels]\n",
    "target_labels\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(14, 8))\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange', 'red', 'black','green', 'orange','pink']\n",
    "for color, i, target_name in zip(colors[:len(target_labels)], list(range(len(target_labels))), target_labels):\n",
    "    plt.scatter(X_r[y_pred == i, 0], X_r[y_pred == i, 1], color=color, alpha=.8, lw=2,label=target_name)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.6)   \n",
    "plt.title(f'PCA of {best_c} Items')\n",
    "plt.savefig('../Images/final_clusters.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5943e8",
   "metadata": {},
   "source": [
    "### 6. Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503697f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_counts = []\n",
    "for i in range(len(target_labels)):\n",
    "    ax = sns.countplot(data=data[data.cluster == i], x='genres')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "    plt.savefig(f'../Images/cluster_analysis_{i}_genres.jpg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59465c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = []\n",
    "for i in range(len(target_labels)):\n",
    "    ax = sns.countplot(data=data[data.cluster == i], x='category')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "    plt.savefig(f'../Images/cluster_analysis_{i}_categories.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_labels)):\n",
    "    print(data[data.cluster == i].primaryname.value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02309eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_labels)):\n",
    "    print(data[data.cluster == i].primarytitle.value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba29f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_labels)):\n",
    "    print(data[data.cluster == i].genres.value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_labels)):\n",
    "    print(np.mean(data[data.cluster == i].numvotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_labels)):\n",
    "    print(np.mean(data[data.cluster == i].startyear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306f5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2f5eeafb4738538680cc9389be83c7f54c751ee609fa97da0b7e381233777c0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
