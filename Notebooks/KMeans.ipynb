{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continent-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d023687",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "The data for this project comes from https://www.imdb.com/interfaces/ as extremely large .tsv (tab-seperated) files, the biggest being over 2GB.\n",
    "I'm handling this by loading each as a pandas dataframe, performing some simple data engineering in order to reduce the data so it doesn't crash my machine or take forever, then saving each as its own .csv file.\n",
    "These files are then read in and deeply wrangled before being merged and saved to a single .csv file.\n",
    "As of right now, there's a bit of SQL at the bottom of this file that I'm tinkering with.\n",
    "Also, the data dictionary on imdb.com is incorrect. I'll provide one once the data has been trimmed down and consolidated.\n",
    "\n",
    "### 1.1 Load & Inspect Each Table\n",
    "'usecols' is a useful parameter for speeding up the reading in of large files because I can specify just the columns I need pandas to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "790a9e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "col_list = ['tconst','nconst','category','ordering']\n",
    "principals = pd.read_csv('../Data/tsv/principals.tsv', sep='\\t',dtype='object', usecols=col_list)\n",
    "\n",
    "col_list = ['tconst','titleType','primaryTitle','startYear','genres']\n",
    "basics = pd.read_table('../Data/tsv/basics.tsv', na_values=['\\\\N','nan'], dtype='object', usecols=col_list)\n",
    "\n",
    "col_list = ['tconst', 'averageRating']\n",
    "ratings = pd.read_table('../Data/tsv/ratings.tsv', low_memory=False, na_values=['\\\\N','nan'], usecols=col_list)\n",
    "\n",
    "col_list = ['nconst', 'primaryName']\n",
    "name = pd.read_table('../Data/tsv/name.tsv', na_values=['\\\\N','nan'], usecols=col_list)\n",
    "\n",
    "col_list = ['titleId','region']\n",
    "akas = pd.read_table('../Data/tsv/akas.tsv', na_values=['\\\\N','nan'], usecols=col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09965c69",
   "metadata": {},
   "source": [
    "#### 1.1.a - Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "basics.info(memory_usage='deep')\n",
    "basics.head()\n",
    "print(basics.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming out tv shows and anything else that's not an actual movie.\n",
    "basics = basics[basics.titleType == 'movie']\n",
    "basics = basics.drop('titleType', axis=1)\n",
    "print(len(basics))\n",
    "\n",
    "# During an earlier view of the data I'd noticed that prior to 2000 there seem to be drastically fewer titles, drawing release year towards a left skew.\n",
    "basics = basics[basics.startYear.between('2000', '2022')]\n",
    "print(len(basics))\n",
    "\n",
    "#TODO just copy data without missing values here.\n",
    "basics.genres.replace('Nan',np.nan, inplace=True)\n",
    "basics.dropna(inplace=True)\n",
    "print(len(basics))\n",
    "\n",
    "# Many of the genre values are combinations of major genres, like drame, romance, and comedy. However, there's a ton of these, so I'll restrict the table to include only the 50 most frequently oberserved generes.\n",
    "genres = basics.genres.value_counts()[:-1]\n",
    "genres = genres[:50]\n",
    "top_genres = genres.index.to_list()\n",
    "basics = basics[basics['genres'].isin(top_genres)]\n",
    "print(len(basics))\n",
    "\n",
    "# Converting to numeric values for analysis.\n",
    "basics['startYear'] = pd.to_numeric(basics.startYear)\n",
    "\n",
    "####################### basics.to_csv('../Data/basics.csv', index=False)\n",
    "basics.info(memory_usage='deep')\n",
    "basics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3877a",
   "metadata": {},
   "source": [
    "#### 1.1.b - Principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353aa30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "principals.info(memory_usage='deep')\n",
    "principals.head()\n",
    "print(principals.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "principals.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fca07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting this table to a set of the most frequent roles.\n",
    "principal_roles = ['actor','actress','director','writer','producer','composer','editor','production_designer']\n",
    "principals['ordering'] = pd.to_numeric(principals.ordering)\n",
    "principals = principals[(principals.ordering == 1) & (principals.category.isin(principal_roles))]\n",
    "print(len(principals))\n",
    "principals.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############principals.to_csv('../Data/principals.csv', index=False)\n",
    "principals.info(memory_usage='deep')\n",
    "principals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fad43",
   "metadata": {},
   "source": [
    "#### 1.1.c - Ratings\n",
    "I only need to scale this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ea8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings.isna().sum().sort_values(ascending=False))\n",
    "############ratings.to_csv('../Data/ratings.csv', index=False)\n",
    "ratings.info(memory_usage='deep')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1aedcd",
   "metadata": {},
   "source": [
    "#### 1.1.d - Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name.isna().sum().sort_values(ascending=False))\n",
    "name.info(memory_usage='deep')\n",
    "name.head()\n",
    "\n",
    "##################### name.to_csv('../Data/name.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898a21",
   "metadata": {},
   "source": [
    "### 1.2 Merging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling casing now that eaach table is ready for the merger.\n",
    "ratings.columns = map(str.lower, ratings.columns)\n",
    "name.columns = map(str.lower, name.columns)\n",
    "principals.columns = map(str.lower, principals.columns)\n",
    "basics.columns = map(str.lower, basics.columns)\n",
    "\n",
    "\n",
    "data = basics.merge(principals, how='left',on='tconst')\n",
    "data = data.merge(ratings, how='left',on='tconst')\n",
    "data = data.merge(name, how='left',on='nconst')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80341561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['tconst','nconst'],axis=1,inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.to_csv('../Data/data.csv', index=False)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5e96d",
   "metadata": {},
   "source": [
    "### 1.3 Table Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbee8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/data.csv')\n",
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data dictionary\n",
    "tconst  =   title id of the movie\n",
    "primarytitle    =   primary title the movie goes by\n",
    "startyear   =   year realease\n",
    "runtimeminutes  =   film duration\n",
    "genres  =   list of each genre the film represents\n",
    "ordering    =   order of precedence if co-directors/writers/producers\n",
    "nconst  =   name id or director, writer\n",
    "category    =   job category7\n",
    "primaryname =   director/writer name gone by\n",
    "primaryprofession   =   primary postion of principal\n",
    "knownfortitles  =   previous works by principle\n",
    "averagerating   =   films average rating\n",
    "numvotes    =   number of votes film has received\n",
    "directors   =   list of directors\n",
    "writers =   list of writers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplcates.\n",
    "data[data.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just doublechecking.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258390a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and Modify columns\n",
    "data.columns\n",
    "# The columns are already formatted to lowercase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea7f31",
   "metadata": {},
   "source": [
    "## 2 Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06cef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ad815",
   "metadata": {},
   "source": [
    "### 2.1 Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814da602",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d221c",
   "metadata": {},
   "source": [
    "#### 2.1.a - primarytitle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9e55d",
   "metadata": {},
   "source": [
    "#### 2.1.b - genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609941a5",
   "metadata": {},
   "source": [
    "#### 2.1.c - category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.category.value_counts())\n",
    "data = data[data.category != 'editor']\n",
    "data = data[data.category != 'production_designer']\n",
    "print(data.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439ee41",
   "metadata": {},
   "source": [
    "#### 2.1.d - primaryname\n",
    "This is a huge source of dimensionality. For now I'm simply going to drop anyone who appears only once. This is hand during the .tsv file conversion process further up but I may fine tune here in the the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f978713",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.primaryname.duplicated(keep=False)]\n",
    "print(data.primaryname.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73158427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "for f in data[['genres','category']]:\n",
    "    sns.countplot(x = f, data = data, palette = 'Set3') # hue = '')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73d51a",
   "metadata": {},
   "source": [
    "### 2.2 Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf405bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('number').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3c6a8",
   "metadata": {},
   "source": [
    "### 2.2.a - startyear\n",
    "- The average start year for the films in this selection is 2009.\n",
    "- This distribution should be plotted with lines indicating centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15081921",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data.startyear)\n",
    "plt.show()\n",
    "sns.kdeplot(data.startyear, shade=True, label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df307",
   "metadata": {},
   "source": [
    "### 2.2.b - averagerating\n",
    "- This is likely to be some sort of target in the future, linear regression would be great to take this project a step further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data.averagerating)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further exploring the outlier impact.\n",
    "sns.kdeplot(data.averagerating, shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy I'll first calculate the IQR, then use it to identify and remove outliers found in the averagerating feature.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#TODO come back and trim this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming off everything above and below the threshold.\n",
    "# Intuition on this dictates that we want data that extreme outliers can lead to groupings - ansd their centroids, being dragged out due to these skewed data.\n",
    "data = data[data.averagerating >= iqr_lower]\n",
    "data = data[data.averagerating <= iqr_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "# we've lost only a small number of rows.\n",
    "#TODO get the original number and show difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out the new distribution using the previous distributions IQR method ranges.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0866173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checking out the new distribution using the new IQR.\n",
    "q1 = np.quantile(data.averagerating, 0.25)\n",
    "q2 = np.quantile(data.averagerating, 0.5)\n",
    "q3 = np.quantile(data.averagerating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(data.averagerating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[data.averagerating > iqr_lower]\n",
    "#data = data[data.averagerating < iqr_upper]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data.averagerating, shade=True, label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out these new summary stats\n",
    "# the max is a more realistic two hours or so while the mean remains about the same. The standard deviation has also been halved.\n",
    "data.averagerating.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-sunrise",
   "metadata": {},
   "source": [
    "#### 2.3 Feature Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=data,x=data.category,y=data.averagerating)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=data,x=data.genres,y=data.averagerating)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.show()\n",
    "#TODO sort this and amke wider for x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09857ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'averagerating'\n",
    "def find_associations(data):\n",
    "    associated = []\n",
    "    for i in data.select_dtypes(np.number).columns:\n",
    "        print(i)\n",
    "        if i == target:\n",
    "            continue\n",
    "        pearson_cor, pval = pearsonr(data[i],data[target])\n",
    "\n",
    "        if pearson_cor > .3:\n",
    "            associated.append([i,pearson_cor])\n",
    "    return associated\n",
    "\n",
    "# To do, there is no target, should I drop this? Or, could it be useful in evaluation..?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60315470",
   "metadata": {},
   "source": [
    "#### 2.2 Diagnose Data\n",
    "Noting a drasting reduction in memory usage while number of datapoints hasn't been as impacted by dropping columns whose values are either irrelevant or outside the project scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(data.duplicated()))\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7910df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-links",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning & Assessment\n",
    "After checking a range of cluster quantities I'm going to use principal component analysis from Sklearn to to reduce the dimensionality of the data. In fact, one hot encoding is used in the next cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = ['genres','category','primaryname']\n",
    "scal_cols = ['startyear','averagerating']\n",
    "X = data\n",
    "evaluations = {}\n",
    "\n",
    "\n",
    "\n",
    "# x_train, x_test = train_test_split(X, test_size=.2, random_state=42)\n",
    "n_digits = 4\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ohe', OneHotEncoder(handle_unknown ='ignore'), ohe),\n",
    "        ('scaler', StandardScaler(), scal_cols)\n",
    "        ],remainder='drop')\n",
    "\n",
    "\n",
    "x_train = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "c_dict = {}\n",
    "n_clusters = [range(1, 11)]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "    kmeans.fit(x_train)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    if k not in c_dict.keys():\n",
    "        c_dict[k] = kmeans.inertia_\n",
    "\n",
    "    print(\"The innertia for :\", k, \"Clusters is:\", kmeans.inertia_)\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distance (Inertia)')\n",
    "plt.title(\"Inertia Plot for k\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96408d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "kmeans.fit(x_train)\n",
    "labels = set(kmeans.labels_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(x_train)\n",
    "kmeans.inertia_\n",
    "old_inertia = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886ea0b",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [f'Cluster {x}' for x in labels]\n",
    "target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train.toarray()\n",
    "y_pred = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d98415",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state = 42)\n",
    "PCA_x_train = pd.DataFrame(x_train.toarray())\n",
    "X_r = pca.fit_transform(PCA_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a65f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['navy', 'turquoise', 'darkorange', 'red', 'black']\n",
    "for color, i, target_name in zip(colors, [0, 1, 2, 3, 4], target_labels):\n",
    "    plt.scatter(X_r[y_pred == i, 0], X_r[y_pred == i, 1], color=color, alpha=.8, lw=2,label=target_name)\n",
    "    \n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.6)   \n",
    "plt.title('PCA of 2 Items')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d335465",
   "metadata": {},
   "source": [
    "Determining Optimal Number of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = x_train.shape[1]\n",
    "pca = PCA(n_components=n_components, random_state = 53)\n",
    "X_r = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_variance = sum(pca.explained_variance_)\n",
    "print('Total Variance in our dataset is: ', total_variance)\n",
    "var_95 = total_variance * 0.95\n",
    "print('The 95% variance we want to have is: ', var_95)\n",
    "print('')\n",
    "# Creating a df with the components and explained variance\n",
    "a = zip(range(0,n_components), pca.explained_variance_)\n",
    "a = pd.DataFrame(a, columns=['PCA Comp', 'Explained Variance'])\n",
    "\n",
    "# Trying to hit 95%\n",
    "\n",
    "d = 1\n",
    "v = []\n",
    "best = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    if len(v) > len(a)*.9:\n",
    "        if sum(v[-5:])/5 == v[:-1]:\n",
    "            break\n",
    "    else:\n",
    "        v.append(sum(a['Explained Variance'][0:d]))\n",
    "        if d%5 == 0:\n",
    "            print(f'Variance explained with {d} compononets: ', sum(a['Explained Variance'][0:d]))\n",
    "            if sum(a['Explained Variance'][0:d]) >= var_95:\n",
    "                best.append((d,sum(a['Explained Variance'][0:d])))\n",
    "        d += 1\n",
    "\n",
    "\n",
    "best_c = best[0][0]\n",
    "best_v = best[0][1]\n",
    "\n",
    "\n",
    "# Plotting the Data\n",
    "plt.figure(1, figsize=(14, 8))\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2, c='r')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_ratio_')\n",
    "\n",
    "# Plotting line with 95% e.v.\n",
    "plt.axvline(best_c,linestyle=':', label='n_components - 95% explained', c ='blue')\n",
    "plt.legend(prop=dict(size=12))\n",
    "\n",
    "# adding arrow\n",
    "plt.annotate(f'{best_c} eigenvectors used to explain 95% variance', xy=(best_c, pca.explained_variance_ratio_[best_c]), \n",
    "             xytext=(best_c+10, pca.explained_variance_ratio_[5]),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'The best is {best_c} components which yeilds {best_v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b464e6",
   "metadata": {},
   "source": [
    "Using PCA with this optimal number of components to add a preprocessing layer to the data before applyin KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "#pca = PCA(n_components=best_c)\n",
    "X_r = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "cs = []\n",
    "n_clusters = [range(1, 11)]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "    kmeans.fit(X_r)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    if k not in c_dict.keys():\n",
    "        c_dict[k] = kmeans.inertia_\n",
    "\n",
    "    print(\"The innertia for :\", k, \"Clusters is:\", kmeans.inertia_)\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distance (Inertia)')\n",
    "plt.title(\"Inertia Plot for k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_r\n",
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 20, n_init = 4, random_state = 42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(X)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_pred'] = y_pred\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.inertia_)\n",
    "print(old_inertia)\n",
    "print(old_inertia-kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"mysql://user:pwd@localhost/kmeans\",echo = True)\n",
    "data.to_sql('kmeans', schema='dbo', con = engine, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb310d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.clustermap(data=data[['startyear','averagerating']])\n",
    "# plt.show()\n",
    "#TODO check out this 'fastcluster' thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc2dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2f5eeafb4738538680cc9389be83c7f54c751ee609fa97da0b7e381233777c0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
